{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e8019f7",
   "metadata": {},
   "source": [
    "# **Home Exercise on Named Entity Recognition**\n",
    "Implement a **Recurrent Neural Network model (Bidirectional LSTM-CRF Models for Sequence Tagging)** to extract named entities from text, entity labels are encoded using the BIO notation, where each entity label is assigned a **B** (Beginning) or **I** (Inside) tag. The **B-** tag indicates the beginning of an entity, while the **I-** tag marks words inside the same entity.\n",
    "\n",
    "These tags help identify multi-word entities. For example, in the phrase **\"World War II\"**, the labels would be: **(B-eve, I-eve, I-eve)**. Words that do not belong to any entity are labeled as **O (Outside)**.\n",
    "\n",
    "* Data: [Annotated GMB Corpus](https://www.kaggle.com/datasets/shoumikgoswami/annotated-gmb-corpus?select=GMB_dataset.txt)(**the last 10% of sentences serve as the test set**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a3a68e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last time this notebook was run is: 08:48:08 21/11/%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import shutil, sys, zipfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from helper_functions import *\n",
    "\n",
    "print(f\"The last time this notebook was run is: {datetime.datetime.now().strftime('%H:%M:%S %d/%m/%')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4bb375e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/shoumikgoswami/annotated-gmb-corpus?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462k/462k [00:00<00:00, 632kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: /home/dikhang/.cache/kagglehub/datasets/shoumikgoswami/annotated-gmb-corpus/versions/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"shoumikgoswami/annotated-gmb-corpus\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "016dc08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved to: ./data/GMB_dataset.txt\n"
     ]
    }
   ],
   "source": [
    "src_dir = \"/home/dikhang/.cache/kagglehub/datasets/shoumikgoswami/annotated-gmb-corpus/versions/1\"\n",
    "filename = \"GMB_dataset.txt\"\n",
    "\n",
    "full_path = os.path.join(src_dir, filename)\n",
    "file_path = move_file(full_path, \"./data\")\n",
    "\n",
    "print(\"Moved to:\", file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa01751",
   "metadata": {},
   "source": [
    "## Loading to Dataset class by DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d503dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "PAD_TAG = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec5495f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(path) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    sentences, tags = [], []\n",
    "    words, labels = [], []\n",
    "    \n",
    "    with open(path, 'r', encoding='utf8', errors='replace') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if words:\n",
    "                    sentences.append(words)\n",
    "                    tags.append(labels)\n",
    "                    words, labels = [], []\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) == 1:\n",
    "                token = parts[0]\n",
    "                tag = \"O\"\n",
    "            else:\n",
    "                token = parts[0]\n",
    "                tag = parts[-1]\n",
    "            words.append(token)\n",
    "            labels.append(tag)\n",
    "    if words:\n",
    "        sentences.append(words)\n",
    "        tags.append(labels)\n",
    "    return sentences, tags\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee678aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 1\n"
     ]
    }
   ],
   "source": [
    "all_sentences, all_tags = read_text(file_path)\n",
    "\n",
    "print(f\"Number of sentences: {len(all_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5cc22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(path):\n",
    "    try:\n",
    "        data = pd.read_csv(path, encoding='latin1')\n",
    "    except:\n",
    "        data = pd.read_csv(path, sep='\\t', encoding='latin1')\n",
    "\n",
    "    print(\"Columns in file:\", data.columns)\n",
    "    if 'Sentence #' in data.columns:\n",
    "        data['Sentence #'] = data['Sentence #'].ffill()\n",
    "    else:\n",
    "        print(\"Error: Cannot found 'Sentence #'\")\n",
    "        return [], []\n",
    "    getter = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(), \n",
    "                                              s[\"Tag\"].values.tolist())]\n",
    "    \n",
    "    grouped = data.groupby(\"Sentence #\").apply(getter)\n",
    "    sentences = [ [s[0] for s in sent] for sent in grouped ]\n",
    "    tags = [ [s[1] for s in sent] for sent in grouped ]\n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0e8ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in file: Index(['Unnamed: 0', 'Sentence #', 'Word', 'POS', 'Tag'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5053/4227761253.py:16: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = data.groupby(\"Sentence #\").apply(getter)\n"
     ]
    }
   ],
   "source": [
    "all_sentences, all_tags = read_text(file_path)\n",
    "\n",
    "\n",
    "total_count = len(all_sentences)\n",
    "train_size = int(total_count * 0.8)\n",
    "test_size = int(total_count * 0.1)\n",
    "val_size = total_count - train_size - test_size\n",
    "\n",
    "train_sentences = all_sentences[:train_size]\n",
    "train_tags = all_tags[:train_size]\n",
    "\n",
    "val_sentences = all_sentences[train_size : train_size + val_size]\n",
    "val_tags = all_tags[train_size : train_size + val_size]\n",
    "\n",
    "\n",
    "test_sentences = all_sentences[train_size + val_size :]\n",
    "test_tags = all_tags[train_size + val_size :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
